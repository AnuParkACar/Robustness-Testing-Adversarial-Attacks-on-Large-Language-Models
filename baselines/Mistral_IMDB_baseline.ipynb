{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBKt2ilXiCc4",
        "outputId": "4732576c-9e90-442e-a29b-b09c0eabc83a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub[cli] in /usr/local/lib/python3.11/dist-packages (0.29.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (4.12.2)\n",
            "Collecting InquirerPy==0.3.4 (from huggingface_hub[cli])\n",
            "  Downloading InquirerPy-0.3.4-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting pfzy<0.4.0,>=0.3.1 (from InquirerPy==0.3.4->huggingface_hub[cli])\n",
            "  Downloading pfzy-0.3.4-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (3.0.50)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[cli]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[cli]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[cli]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[cli]) (2025.1.31)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]) (0.2.13)\n",
            "Downloading InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pfzy-0.3.4-py3-none-any.whl (8.5 kB)\n",
            "Installing collected packages: pfzy, InquirerPy\n",
            "Successfully installed InquirerPy-0.3.4 pfzy-0.3.4\n"
          ]
        }
      ],
      "source": [
        "!pip install -U \"huggingface_hub[cli]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jV6xjCjiiNSp",
        "outputId": "613329c5-349a-4e32-ef0b-b74ee47dd1b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: huggingface-cli <command> [<args>]\n",
            "\n",
            "positional arguments:\n",
            "  {download,upload,repo-files,env,login,whoami,logout,auth,repo,lfs-enable-largefiles,lfs-multipart-upload,scan-cache,delete-cache,tag,version,upload-large-folder}\n",
            "                        huggingface-cli command helpers\n",
            "    download            Download files from the Hub\n",
            "    upload              Upload a file or a folder to a repo on the Hub\n",
            "    repo-files          Manage files in a repo on the Hub\n",
            "    env                 Print information about the environment.\n",
            "    login               Log in using a token from huggingface.co/settings/tokens\n",
            "    whoami              Find out which huggingface.co account you are logged in as.\n",
            "    logout              Log out\n",
            "    auth                Other authentication related commands\n",
            "    repo                {create} Commands to interact with your huggingface.co repos.\n",
            "    lfs-enable-largefiles\n",
            "                        Configure your repository to enable upload of files > 5GB.\n",
            "    scan-cache          Scan cache directory.\n",
            "    delete-cache        Delete revisions from the cache directory.\n",
            "    tag                 (create, list, delete) tags for a repo in the hub\n",
            "    version             Print information about the huggingface-cli version.\n",
            "    upload-large-folder\n",
            "                        Upload a large folder to a repo on the Hub\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli --help"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4C_avlYpYk-C",
        "outputId": "dc768b34-0acd-474a-e2ff-416bad399bb1"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): "
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQlAEuB0QPM8"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers datasets tqdm evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "he6hzQ9zU9Eh"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hry9Lgj9XcCS"
      },
      "outputs": [],
      "source": [
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzbAnWArPBOp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZDNHHK8UKZC"
      },
      "outputs": [],
      "source": [
        "# MODEL_NAME = \"NousResearch/Llama-2-7b-hf\"\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "NUM_EXAMPLES = 1000  # (for 15GB of TPU RAM, around 4600 samples get processed in 2.5 hrs before GPU usage runs out)\n",
        "MAX_NEW_TOKENS = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1i2ankscULi7"
      },
      "outputs": [],
      "source": [
        "print(f\"Loading model: {MODEL_NAME}\")\n",
        "\n",
        "# for using mistral-7B\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciWeUEN1uqu0"
      },
      "outputs": [],
      "source": [
        "def format_prompt(review):\n",
        "    return f\"<s>[INST] Is the following movie review positive or negative?\\n\\nReview: {review} [/INST]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcyzjok8vKX3"
      },
      "outputs": [],
      "source": [
        "def extract_label(generated):\n",
        "    answer = generated.split(\"[/INST]\")[-1].strip().lower()\n",
        "    if \"positive\" in answer:\n",
        "        return 1\n",
        "    elif \"negative\" in answer:\n",
        "        return 0\n",
        "    return -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eWA3aBbvO9_"
      },
      "outputs": [],
      "source": [
        "print(\"Loading IMDB dataset...\")\n",
        "dataset = load_dataset(\"imdb\", split=f\"test\")\n",
        "dataset = dataset.shuffle(seed=42)\n",
        "dataset = dataset.select(range(NUM_EXAMPLES))\n",
        "print(f\"Loaded {len(dataset)} examples.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-nZMH_VIj4Y"
      },
      "outputs": [],
      "source": [
        "y_true, y_pred = [], []\n",
        "\n",
        "for item in tqdm(dataset):\n",
        "    label = item[\"label\"]\n",
        "    review = item[\"text\"]\n",
        "\n",
        "    prompt = format_prompt(review)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=MAX_NEW_TOKENS,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    pred_label = extract_label(output_text)\n",
        "\n",
        "    if pred_label != -1:\n",
        "        y_true.append(label)\n",
        "        y_pred.append(pred_label)\n",
        "\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "print(f\"\\nAccuracy on IMDB: {acc * 100:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}